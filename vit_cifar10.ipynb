{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdZiyoT8I2HOPvhEQKBxKS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtDowdy/deep-learning-engagement/blob/main/vit_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPmnv5vw9BYX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Vision Transformer (from scratch) on CIFAR-10 (PyTorch, Colab-ready)\n",
        "# - Patch embedding via Conv2d\n",
        "# - Class token + learnable positional embeddings\n",
        "# - TransformerEncoder (multihead self-attention)\n",
        "# - AMP training, cosine schedule, label smoothing, early stopping\n",
        "# - Metrics: Top-1 accuracy; Confusion Matrix\n",
        "# - Exports: best checkpoint + TorchScript\n",
        "# - Attention rollout visualization for 1 sample\n",
        "# ============================================================\n",
        "\n",
        "import math, os, time, random\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils as tvutils\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "@dataclass\n",
        "class CFG:\n",
        "    img_size: int = 32\n",
        "    patch_size: int = 4          # 32x32 -> 8x8 = 64 patches\n",
        "    in_chans: int = 3\n",
        "    num_classes: int = 10\n",
        "    embed_dim: int = 192         # ViT-Tiny-ish\n",
        "    depth: int = 6\n",
        "    num_heads: int = 3\n",
        "    mlp_ratio: float = 4.0\n",
        "    dropout: float = 0.1\n",
        "    attn_dropout: float = 0.1\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 30\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.05\n",
        "    early_patience: int = 5\n",
        "    label_smoothing: float = 0.05\n",
        "    num_workers: int = 2\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = CFG()\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "seed_everything(cfg.seed)\n",
        "\n",
        "# ---------------- Data ----------------\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomCrop(cfg.img_size, padding=4, padding_mode=\"reflect\"),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n",
        "])\n",
        "\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "data_root = \"./data\"\n",
        "train_set = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tfms)\n",
        "test_set  = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
        "                          num_workers=cfg.num_workers, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=cfg.batch_size, shuffle=False,\n",
        "                          num_workers=cfg.num_workers, pin_memory=True)\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = self.grid_size * self.grid_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        x = self.proj(x)  # (B, E, H/P, W/P)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, N, E)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x); x = self.act(x); x = self.drop(x)\n",
        "        x = self.fc2(x); x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, batch_first=True)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "    def forward(self, x, need_attn=False):\n",
        "        h = self.norm1(x)\n",
        "        attn_out, attn_map = self.attn(h, h, h, need_weights=need_attn, average_attn_weights=False)\n",
        "        x = x + self.drop(attn_out)\n",
        "        h = self.norm2(x)\n",
        "        x = x + self.drop(self.mlp(h))\n",
        "        return (x, attn_map) if need_attn else x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_chans, num_classes,\n",
        "                 embed_dim, depth, num_heads, mlp_ratio, drop, attn_drop):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim) * 0.02)\n",
        "        self.pos_drop = nn.Dropout(drop)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, drop, attn_drop) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
        "        nn.init.zeros_(self.head.bias)\n",
        "\n",
        "    def forward_features(self, x, collect_attn=False):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)                    # (B, N, E)\n",
        "        cls = self.cls_token.expand(B, -1, -1)     # (B, 1, E)\n",
        "        x = torch.cat([cls, x], dim=1)             # (B, 1+N, E)\n",
        "        x = x + self.pos_embed[:, :x.size(1), :]\n",
        "        x = self.pos_drop(x)\n",
        "        attn_maps = []\n",
        "        for blk in self.blocks:\n",
        "            if collect_attn:\n",
        "                x, a = blk(x, need_attn=True)\n",
        "                attn_maps.append(a)  # (B, heads, T, T)\n",
        "            else:\n",
        "                x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return (x[:, 0], attn_maps) if collect_attn else x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# label smoothing CE\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, smoothing=0.0):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, logits, target):\n",
        "        n = logits.size(-1)\n",
        "        logp = F.log_softmax(logits, dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(logp)\n",
        "            true_dist.fill_(self.smoothing / (n - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n",
        "        return torch.mean(torch.sum(-true_dist * logp, dim=-1))\n",
        "\n",
        "model = ViT(cfg.img_size, cfg.patch_size, cfg.in_chans, cfg.num_classes,\n",
        "            cfg.embed_dim, cfg.depth, cfg.num_heads, cfg.mlp_ratio,\n",
        "            cfg.dropout, cfg.attn_dropout).to(device)\n",
        "\n",
        "# ---------------- Optim & Sched ----------------\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "criterion = LabelSmoothingCE(cfg.label_smoothing)\n",
        "\n",
        "# ---------------- Train/Eval ----------------\n",
        "best_acc = 0.0\n",
        "best_state = None\n",
        "pat = 0\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            ys.append(y.cpu().numpy()); ps.append(pred.cpu().numpy())\n",
        "    ys = np.concatenate(ys); ps = np.concatenate(ps)\n",
        "    acc = accuracy_score(ys, ps)\n",
        "    cm = confusion_matrix(ys, ps)\n",
        "    return acc, cm\n",
        "\n",
        "for epoch in range(1, cfg.epochs+1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    t0 = time.time()\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running += loss.item() * x.size(0)\n",
        "    scheduler.step()\n",
        "\n",
        "    train_loss = running / len(train_set)\n",
        "    acc, cm = evaluate(model, test_loader)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Epoch {epoch:02d} | loss={train_loss:.4f} | test_acc={acc:.3f} | time={dt:.1f}s\")\n",
        "\n",
        "    if acc > best_acc + 1e-4:\n",
        "        best_acc = acc\n",
        "        best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n",
        "        pat = 0\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= cfg.early_patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# restore best\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k: v.to(device) for k,v in best_state.items()})\n",
        "print(f\"Best test accuracy: {best_acc:.3f}\")\n",
        "\n",
        "# ---------------- Save & Export ----------------\n",
        "os.makedirs(\"artifacts_vit\", exist_ok=True)\n",
        "ckpt_path = \"artifacts_vit/vit_cifar10_best.pt\"\n",
        "torch.save({\"state_dict\": model.state_dict(), \"cfg\": cfg.__dict__}, ckpt_path)\n",
        "print(\"Saved checkpoint:\", ckpt_path)\n",
        "\n",
        "# TorchScript export\n",
        "model.eval()\n",
        "example = torch.randn(1, 3, cfg.img_size, cfg.img_size).to(device)\n",
        "scripted = torch.jit.trace(model, example)\n",
        "ts_path = \"artifacts_vit/vit_cifar10_scripted.pt\"\n",
        "scripted.save(ts_path)\n",
        "print(\"Saved TorchScript:\", ts_path)\n",
        "\n",
        "# ---------------- Attention Rollout Viz ----------------\n",
        "# Grab one test image; compute attention maps and rollout\n",
        "inv_norm = transforms.Normalize(\n",
        "    mean=[-m/s for m,s in zip(mean, std)],\n",
        "    std=[1/s for s in std]\n",
        ")\n",
        "\n",
        "def attention_rollout(attn_maps):\n",
        "    # attn_maps: list of (B, heads, T, T), T=1+N\n",
        "    # Average heads, apply residual & multiply across layers\n",
        "    with torch.no_grad():\n",
        "        attn = torch.stack(attn_maps, dim=0).mean(2)  # (L, B, T, T)\n",
        "        I = torch.eye(attn.size(-1), device=attn.device).unsqueeze(0).unsqueeze(1)\n",
        "        attn = attn + I  # add residual\n",
        "        attn = attn / attn.sum(-1, keepdim=True)\n",
        "        joint = attn[0]\n",
        "        for i in range(1, attn.size(0)):\n",
        "            joint = torch.bmm(attn[i], joint)\n",
        "        # influence from CLS to patches (exclude CLS itself)\n",
        "        cls_to_patch = joint[:, 0, 1:]  # (B, N)\n",
        "        return cls_to_patch\n",
        "\n",
        "# one sample\n",
        "x_img, _ = next(iter(test_loader))\n",
        "x_img = x_img[:1].to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model.patch_embed(x_img)  # warm run\n",
        "    _, attn_maps = model.forward_features(x_img, collect_attn=True)\n",
        "\n",
        "roll = attention_rollout(attn_maps).reshape(1, model.patch_embed.grid_size, model.patch_embed.grid_size)\n",
        "roll = F.interpolate(roll.unsqueeze(1), size=(cfg.img_size, cfg.img_size), mode=\"bilinear\", align_corners=False)\n",
        "roll = roll.squeeze().cpu().numpy()\n",
        "roll = (roll - roll.min()) / (roll.max() - roll.min() + 1e-8)\n",
        "\n",
        "# save overlay\n",
        "img_vis = inv_norm(x_img[0].cpu()).clamp(0,1)\n",
        "grid = tvutils.make_grid(img_vis, nrow=1)\n",
        "grid_np = grid.permute(1,2,0).numpy()\n",
        "heat = np.uint8(255 * roll)\n",
        "import PIL.Image as Image\n",
        "import matplotlib.cm as cm\n",
        "heatmap = cm.jet(heat/255.0)[...,:3]\n",
        "overlay = (0.55*grid_np + 0.45*heatmap).clip(0,1)\n",
        "Image.fromarray(np.uint8(overlay*255)).save(\"artifacts_vit/attention_rollout.png\")\n",
        "print(\"Saved attention rollout: artifacts_vit/attention_rollout.png\")\n"
      ]
    }
  ]
}